---
layout: ../layouts/Layout.astro
title: "CLIP-RT : Learning Language-Conditioned Robotic Policies from Natural Language Supervision"
description: Learning Language-Conditioned Robotic Policies from Natural Language Supervision
favicon: /favicon.svg
thumbnail: screenshot.png
---

import "../styles/index.css";

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";


import close_the_laptop from "../assets/videos/close_the_laptop.mp4";
import draw_a_line from "../assets/videos/draw_a_line.mp4";
import erase_the_whiteboard from "../assets/videos/erase_the_whiteboard.mp4";
import hide_the_pooh from "../assets/videos/hide_the_pooh.mp4";
import open_the_cabinet from "../assets/videos/open_the_cabinet.mp4";
import open_the_trashcan from "../assets/videos/open_the_trashcan.mp4";
import play_with_car from "../assets/videos/play_with_car.mp4";
import pour_the_dog_food from "../assets/videos/pour_the_dog_food.mp4";
import stamp from "../assets/videos/stamp.mp4";


import snu_ipai from "../assets/snu_ipai.png";
import snu_aiis from "../assets/snu_aiis.png";
import imit from "../assets/imit.png";
import model_overview from "../assets/model_overview.png";
import std from "../assets/std.png";
import exp_results from "../assets/exp_results.png";
import exp_results2 from "../assets/exp_results2.png";
import figure1 from "../assets/figure1.png";
import tsne from "../assets/TSNE.png";

import Splat from "../components/Splat.tsx"

import CodeBlock from "../components/CodeBlock.astro";
export const components = {pre: CodeBlock}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Gi-Cheon Kang",
      url: "https://gicheonkang.com",
      notes: ["*"],
    },
    {
      name: "Junghyun Kim",
      url:"https://jhkim-snu.github.io/",
      notes: ["*"],
    },
    {
      name: "Kyuhwan Shim",
      url:"https://underthelights.github.io/",
      notes: [""],
    },
    {
      name: "Jun Ki Lee",
      url:"https://junkilee.github.io/",
      notes: ["†"],
    },
    {
      name: "Byoung-Tak Zhang",
      url:"https://bi.snu.ac.kr/~btzhang/",
      notes: ["†"],
    },
  ]}
  institute={[
    { symbol: "", name: "Seoul National University" },
  ]} 
  notes={[
    {
      symbol: "*",
      text: "Equal Contribution",
    },
    {
      symbol: "†",
      text: "Corresponding Authors",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "https://arxiv.org/abs/2411.00508",
      icon: "fa-solid:file-pdf",
    },
    {
      name: "Code",
      url: "https://github.com/gicheonkang/clip-rt",
      icon: "mdi:github",
    },
  ]}
/>
<section class="hero is-light is-small">
  <div class="container is-max-desktop has-text-centered">
    <div class="hero-body">
      <div class="video-group">
        <figure>
          <video id="close_the_laptop" autoplay muted loop playsinline>
            <source src={close_the_laptop} type="video/mp4" />
          </video>
          <figcaption>Close the laptop</figcaption>
        </figure>
        <figure>
          <video id="draw_a_line" autoplay muted loop playsinline>
            <source src={draw_a_line} type="video/mp4" />
          </video>
          <figcaption>Draw a line from the star to the circle</figcaption>
        </figure>
        <figure>
          <video id="erase_the_whiteboard" autoplay muted loop playsinline>
            <source src={erase_the_whiteboard} type="video/mp4" />
          </video>
          <figcaption>Erase the whiteboard</figcaption>
        </figure>
        <figure>
          <video id="hide_the_pooh" autoplay muted loop playsinline>
            <source src={hide_the_pooh} type="video/mp4" />
          </video>
          <figcaption>Hide the Pooh with the green cup</figcaption>
        </figure>
        <figure>
          <video id="open_the_cabinet" autoplay muted loop playsinline>
            <source src={open_the_cabinet} type="video/mp4" />
          </video>
          <figcaption>Open the cabinet</figcaption>
        </figure>
        <figure>
          <video id="open_the_trashcan" autoplay muted loop playsinline>
            <source src={open_the_trashcan} type="video/mp4" />
          </video>
          <figcaption>Open the trash can</figcaption>
        </figure>
        <figure>
          <video id="play_with_car" autoplay muted loop playsinline>
            <source src={play_with_car} type="video/mp4" />
          </video>
          <figcaption>Play with the orange car</figcaption>
        </figure>
        <figure>
          <video id="pour_the_dog_food" autoplay muted loop playsinline>
            <source src={pour_the_dog_food} type="video/mp4" />
          </video>
          <figcaption>Pour the dog food in the bowl</figcaption>
        </figure>
        <figure>
          <video id="stamp" autoplay muted loop playsinline>
            <source src={stamp} type="video/mp4" />
          </video>
          <figcaption>Stamp near the circle</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<section class>
  <div class="box" style="background-color: #f5f5f5; border-left: 4px solid #3273dc; padding: 10px; font-size: 0.5rem;">
      <p><strong>TL;DR</strong></p>
      <p>We explore how non-experts can teach robotic skills only through natural language supervision.</p>
      <p>We propose a VLA model that learns visuomotor policies directly from this supervision.</p>
  </div>
</section>

<HighlightedSection>
## Abstract

Teaching robots desired skills in real-world environments remains challenging, especially for non-experts. A key bottleneck is that collecting robotic data often requires expertise or specialized hardware, limiting accessibility and scalability. We posit that natural language offers an intuitive and accessible interface for robot learning. To this end, we study two aspects: (1) enabling non-experts to collect robotic data through natural language supervision (e.g., "move the arm to the right") and (2) learning robotic policies directly from this supervision. Specifically, we introduce a data collection framework that collects robot demonstrations based on natural language supervision and further augments these demonstrations. We then present CLIP-RT, a vision-language-action (VLA) model that learns language-conditioned visuomotor policies from this supervision. CLIP-RT adapts the pretrained CLIP models and learns to predict language-based motion primitives via contrastive imitation learning. We train CLIP-RT on the Open X-Embodiment dataset and finetune it on in-domain data collected by our framework to learn diverse skills. CLIP-RT demonstrates strong capabilities in learning novel manipulation skills, outperforming the state-of-the-art model, OpenVLA (7B parameters), by 24% in average success rates, while using 7x fewer parameters (1B). We further observe that CLIP-RT shows significant improvements in few-shot generalization. Finally, through collaboration with humans or large pretrained models, CLIP-RT can further improve its generalization on challenging robotic tasks.


</HighlightedSection>

## Motivation

Recent work has made significant progress toward generalist robotic policies using large-scale demonstration datasets. However, collecting these demonstrations often requires expertise and access to specialized hardware (e.g., teleoperation systems), limiting the accessibility and scalability of robot learning. How can non-experts train robotic policies without relying on specialized expertise or devices for data collection?  

**We already have an intuitive and accessible interface for robot learning: natural language**.
In this work, we explore an approach for training robotic skills solely through natural language. It includes:  

<section class>
  <div class="box" style="background-color: #f5f5f5; border-left: 4px solid #3273dc; padding: 10px; font-size: 0.5rem; margin-left: 0;">
      <p>Robotic data collection through natural language supervision</p>
      <p>A VLA model that learns visuomotor skills directly from natural language supervision</p>
  </div>
</section>


## Data Collection

### Language-Based Teleoperation

<Figure
    caption="Language-based teleoperation">
    <Image source={figure1} altText="Research Question" />
</Figure>

We propose a language-based teleoperation method which leverages the in-context learning capabilities of large language models (LLMs). We collect **10 episodes** for each skill through language-based teleoperation

<section class>
  <div class="box" style="background-color: #f5f5f5; border-left: 4px solid #3273dc; padding: 10px; font-size: 0.5rem; margin-left: 0;">
      <p>1. Provide initial **language instruction** (e.g., “Pour the dog food into the bowl”)</p>
      <p>2. Provide **natural language supervision** in each state to complete the instruction</p>
      <p>3. LLM translates the language supervision into the **end-effector action** based on the text prompt </p>
      <p>4. Repeat step 2 and step 3 until the episode ends</p>
  </div>
</section>


### Stochastic Trajectory Diversification (STD)

<Figure
    caption="Stochastic Trajectory Diversification">
    <Image source={std} altText="Stochastic Trajectory Diversification" />
</Figure>

We augment the demonstration data collected by humans based on a method called stochastic trajectory diversification (STD), consisting of two parts: 

(1) **The Diversification phase** diversifies the expert trajectory into multiple alternative trajectories

(2) **The Recovery phase** intentionally deviates from the original trajectory and then executes a recovery action to return to the original path. Recovery action is utilized in training


## CLIP-RT Model 

<Figure
    caption="Overview of CLIP-RT.">
    <Image source={model_overview} altText="Overview of CLIP-RT." />
</Figure>

We propose a new VLA model, CLIP-RT which extends the idea of CLIP to robot learning to learn language-conditioned policies from natural language. Different from other VLA models, CLIP-RT learns to predict **robotic actions represented in natural language** (e.g., “Move arm left”) based on contrastive imitation learning. Furthermore, CLIP-RT is a **discriminative VLA model** that predicts the language-based motion primitive in the predefined list of motion primitives.


### Contrastive Imitation Learning

The goal of contrastive imitation learning is to optimize the pairwise similarity between language supervision and contextual information (i.e., the current scene and language instruction). We first train CLIP-RT on Open X-Embodiment (OXE) dataset and then finetune it on our collected in-domain data. Since the OXE dataset does not contain natural language supervision, we transform existing low-level end-effector actions into natural language supervision to train CLIP-RT.


### Closed-Loop Robot Control

In each time step, CLIP-RT selects the language-based motion primitive with the highest similarity score. The selected motion primitive is translated into the lower-level end-effector commands based on a pre-defined lookup table. Since CLIP-RT is a discriminative model, CLIP-RT can predict action in a single forward pass without autoregressive decoding. This model requires 7GB of GPU memory and runs at 16Hz (one H100 GPU using float32 precision) and 8Hz (one NVIDIA RTX 3090 GPU using float32 precision) without applying any speed-up tricks, such as model quantization and compilation.  



## Experiments 

<Figure
    caption="Main results on 18 robot manipulation tasks">
    <Image source={exp_results} altText="Experimental results on 18 robot manipulation tasks." />
</Figure>

We evaluate CLIP-RT on 9 Common tasks (Top) and 9 Novel tasks (Bottom); Arranged in ascending order based on average steps per episode.

<section class>
  <div class="box" style="background-color: #f5f5f5; border-left: 4px solid #3273dc; padding: 10px; font-size: 0.5rem; margin-left: 0;">
      <p><strong>Key Takeaways</strong></p>
      <p>1. CLIP-RT outperforms OpenVLA in average success rates by **3% (Common) and 24% (Novel)**.</p>
      <p>2. **Natural language supervision** significantly increases performance (vs CLIP-RT-Action)</p>
      <p>3. **STD** boosts the overall performance (vs CLIP-RT-Passive)</p>
  </div>
</section>


### Why does CLIP-RT perform well on Novel manipulation tasks?  

<Figure
    caption="Comparison between multi-task and single-task policies">
    <Image source={exp_results2} altText="Comparison between multi-task and single-task policies." style={{ width: '300px', height: 'auto' }}/>
</Figure>

Where does the significant performance gap between CLIP-RT and OpenVLA on Novel tasks come from? One of our hypotheses is that CLIP-RT effectively learns the shared structure across diverse robotic tasks by utilizing language-based motion primitives as basic building blocks. To verify this, we train a single-task policy for each Novel task and evaluate the performance of each model. OpenVLA-Single and CLIP-RT-Single denote the performance of single-task policies for each model. CLIP-RT benefits more on multi-task policies (+11.1%) compared with OpenVLA (+3.3%). It indicates that CLIP-RT benefits more from shared knowledge across diverse robotic tasks. In other words, learning more generalizable and transferable representations is one of the factors that CLIP-RT performs well on Novel tasks. 


Another factor is attributed to the use of natural language supervision. As shown in the main results, CLIP-RT consistently outperforms a baseline model, CLIP-RT-Action that learns action tokens from scratch similar to existing VLA models. This observation implies that the use of natural language supervision enhances CLIP-RT’s generalization capabilities. To delve into this more deeply, we visualize how these models embed each motion primitives.    

<Figure
    caption="t-SNE visualization of 58 motion primitive embeddings from CLIP-RT (left) and CLIP-RT-Action (right)}.">
    <Image source={tsne} altText="t-SNE visualization of 58 motion primitive embeddings from CLIP-RT (left) and CLIP-RT-Action (right)}."/>
</Figure>

The figure above shows the t-SNE projection of 58 motion primitives for CLIP-RT (left) and CLIP-RT-Action (right). We categorize motion primitives into 16 groups based on the type of displacement. As shown in the Figure, CLIP-RT tends to embed the same groups of motions closer, while CLIP-RT-Action does not show clear structures in its embeddings. It implies that natural language supervision enables CLIP-RT to leverage inherent language priors in the pretrained vision-language model (i.e., CLIP), facilitating the learning of more structured and semantically meaningful action representations. We conjecture that these language priors improve the generalization capabilities of CLIP-RT. 


Our paper describes other interesting results, such as (1) collaborating CLIP-RT with humans or large pretrained models and (2) few-shot generalization capabilities.  
   

## Conclusion
This paper presents CLIP-RT, enabling non-experts to teach robots new manipulation skills through natural language, making robot learning more accessible and scalable for everyday users.


## BibTeX citation

```bibtex
@article{kang2024cliprt,
  title={CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural Language Supervision},
  author={Kang, Gi-Cheon and Kim, Junghyun and Shim, Kyuhwan and Lee, Jun Ki and Zhang, Byoung-Tak},
  journal={arXiv preprint arXiv:2411.00508},
  year = {2024}
}
```

## Acknowledgements
This work was partly supported by the IITP (RS-2021-II212068-AIHub/10%, RS-2021-II211343-GSAI/20%, 2022-0-00951-LBA/20%, 2022-0-00953-PICA/20%) and NRF (RS-2024-00353991/20%, RS-2023-00274280/10%) grant funded by the Korean government.
