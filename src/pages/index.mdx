---
layout: ../layouts/Layout.astro
title: "CLIP-RT : Learning Language-Conditioned Robotic Policies from Natural Language Supervision"
description: Learning Language-Conditioned Robotic Policies from Natural Language Supervision
favicon: favicon.ico
thumbnail: screenshot.png
---

import "../styles/index.css";
import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";


import close_the_laptop from "../assets/videos/close_the_laptop.mp4";
import draw_a_line from "../assets/videos/draw_a_line.mp4";
import erase_the_whiteboard from "../assets/videos/erase_the_whiteboard.mp4";
import hide_the_pooh from "../assets/videos/hide_the_pooh.mp4";
import open_the_cabinet from "../assets/videos/open_the_cabinet.mp4";
import open_the_trashcan from "../assets/videos/open_the_trashcan.mp4";
import play_with_car from "../assets/videos/play_with_car.mp4";
import pour_the_dog_food from "../assets/videos/pour_the_dog_food.mp4";
import stamp from "../assets/videos/stamp.mp4";


import snu_ipai from "../assets/snu_ipai.png";
import snu_aiis from "../assets/snu_aiis.png";

import model_overview from "../assets/model_overview.png";
import std from "../assets/std.png";
import exp_results from "../assets/exp_results.png";
import figure1 from "../assets/figure1.png";

import Splat from "../components/Splat.tsx"

import CodeBlock from "../components/CodeBlock.astro";
export const components = {pre: CodeBlock}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Gi-Cheon Kang",
      url: "https://gicheonkang.com",
      notes: ["1,2*"],
    },
    {
      name: "Junghyun Kim",
      url:"https://jhkim-snu.github.io/",
      notes: ["1,2*"],
    },
    {
      name: "Kyuhwan Shim",
      url:"https://bi.snu.ac.kr/~khshim/",
      notes: ["1,2"],
    },
    {
      name: "Jun Ki Lee",
      url:"https://junkilee.github.io/",
      notes: ["2†"],
    },
    {
      name: "Byoung-Tak Zhang",
      url:"https://bi.snu.ac.kr/~btzhang/",
      notes: ["1,2†"],
    },
  ]}
  institute={[
    { symbol: "¹", name: "IPAI, Seoul National University" },
    { symbol: "²", name: "AI Institute, Seoul National University (AIIS)" },
  ]} 
  notes={[
    {
      symbol: "*",
      text: "Equal Contribution",
    },
    {
      symbol: "†",
      text: "Corresponding Authors",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "https://sites.google.com/view/langrob-corl24/home?authuser=0",
      icon: "fa-solid:file-pdf",
    },
    {
      name: "Code",
      url: "https://github.com/JHKim-snu/CLIP-RT",
      icon: "mdi:github",
    },
    {
      name: "arXiv",
      url: "https://arxiv.org/",
      icon: "academicons:arxiv",
    },
  ]}
/>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div class="video-group">
        <Figure caption="Close the laptop">
          <Video source={close_the_laptop} />
        </Figure>
        <Figure caption="Draw a line from the star to the circle">
          <Video source={draw_a_line} />
        </Figure>
        <Figure caption="Erase the whiteboard">
          <Video source={erase_the_whiteboard} />
        </Figure>
        <Figure caption="Hide the Pooh with the green cup">
          <Video source={hide_the_pooh} />
        </Figure>
        <Figure caption="Open the cabinet">
          <Video source={open_the_cabinet} />
        </Figure>
        <Figure caption="Open the trash can">
          <Video source={open_the_trashcan} />
        </Figure>
        <Figure caption="Play with the orange car">
          <Video source={play_with_car} />
        </Figure>
        <Figure caption="Pour the dog food in the bowl">
          <Video source={pour_the_dog_food} />
        </Figure>
        <Figure caption="Stamp near the circle">
          <Video source={stamp} />
        </Figure>
      </div>
    </div>
  </div>
</section>
 

<section class>
  <div class="box" style="background-color: #f5f5f5; border-left: 4px solid #3273dc; padding: 10px; font-size: 0.5rem;">
      <p><strong>TL;DR</strong></p>
      <p>We explore how non-experts can train robotic skills only through natural language supervision.</p>
      <p>Our VLA model, CLIP-RT, learns end-to-end policies directly from this supervision.</p>
  </div>
</section>

<HighlightedSection>
## Abstract

This paper explores how non-experts can teach robots desired skills in their environments. We argue that natural language is an intuitive and accessible interface for robot learning. 
To this end, we investigate two key aspects: (1) how non-experts collect robotic data using natural language supervision and (2) how pre-trained vision-language models learn end-to-end policies directly from this supervision. 
We propose a data collection framework that collects robot demonstrations based on natural language supervision (e.g., ``move forward'') and further augments these demonstrations. 
Next, we introduce a model that learns language-conditioned policies from natural language supervision called CLIP-RT. 
Our model employs pre-trained CLIP models and learns to predict actions represented in language via contrastive imitation learning. 
We first train CLIP-RT on large-scale robotic data and then enable it to learn desired skills using data collected from our framework.
CLIP-RT shows strong capabilities in acquiring novel manipulation skills, outperforming the state-of-the-art model, OpenVLA (7B parameters), by 17\% in average success rates, while using 7x fewer parameters (1B). 

</HighlightedSection>

## Motivation

### Challenges in Language-Conditioned Robotic Policies
- Obtaining real-world robot data often relies on experts who can operate robots or teleoperation systems
- Struggle to rapidly expand a set of manipulation skills to perform a wide range of real-world tasks

### Research Question

How non-experts can teach robots desired skills intuitively by using natural language as a interface?


<Figure
    caption="Language-based teleoperation">
    <Image source={figure1} altText="Research Question" />
</Figure>


### Approach
- Collect robotic data using natural language
- Train using collected data

## Data Collection

### Language-Based Teleoperation

1. Provide initial **language instruction** (e.g., “ Pour the dog food ”)
2. Provide **natural language supervisions** in specific states to complete the instruction (e.g., “ Move left a lot ”)
3. LLMs translate the natural language supervision into the **low-level end-effector command** based on a detailed text prompt :
    1. input and output space
    2. the 3D Cartesian coordinate system of the environment
    3. input-output examples


- Collect **10 episodes** for each skill via LLM-based teleoperation


### Stochastic Trajectory Diversification (STD)

<Figure
    caption="Stochastic Trajectory Diversification">
    <Image source={std} altText="Stochastic Trajectory Diversification" />
</Figure>
- **The Diversification phase** diversifies the expert trajectory into multiple alternative trajectories
- **The Recovery phase** intentionally deviates from the original trajectory and then executes a recovery action to return to the original path. Recovery action is utilized in training


## CLIP-RT Model 

The CLIP-RT model extends the idea of CLIP to robot learning to learn language-conditioned robotic policies from natural language


<Figure
    caption="Overview of CLIP-RT.">
    <Image source={model_overview} altText="Overview of CLIP-RT." />
</Figure>

**Contrastive Imitation Learning**
- Learns to measure the pairwise similarity between language supervision and contextual information
(i.e., current scene and language instruction)

**Training Data**
- Open X-Embodiment data / collected in-domain data

**Closed-Loop Robot Control**
- In each time step, CLIP-RT selects the action class with the highest similarity score between the context and
￼ action classes specified in natural language 

<div class="latex-container">
  <LaTeX formula="\begin{aligned}
  \mathcal{L}_{\mathrm{cil}} = & - {1 \over M^2} \sum_{i=1}^M \sum_{j=1}^M \left[y_{ij} \mathrm{log} \sigma(s_{ij}) + (1 - y_{ij}) \mathrm{log} (1-\sigma(s_{ij})) \right]; \\
  \sigma(s_{ij}) = & \frac{1}{1 + \mathrm{exp}(-\mathrm{sim}(c_i, z_j))}
  \end{aligned}" />
</div>

## Experimental Results 

<Figure
    caption="Experimental Results">
    <Image source={exp_results} altText="Experimental Results" />
</Figure>
- Evaluate on 9 Common tasks (Top) and 10 Novel tasks (Bottom);
Arranged in ascending order based on average steps per episode

### Key Takeaways

- CLIP-RT outperforms OpenVLA in average success rates by **3%
on common tasks and 17% on novel manipulation tasks**
- Vision-language models (VLMs) trained on Internet-scale data
favor **action representations specified in language**, rather than
existing low-level action tokenization (v.s. CLIP-RT-Action)
- **Impact of STD**: CLIP-RT v.s. CLIP-RT-Passive

## Conclusion
This paper presents CLIP-RT, enabling non-experts to teach robots new manipulation skills through natural language, making robot learning more accessible and scalable for everyday users.

## BibTeX citation

```bibtex
@article{kang2024cliprt,
  title={CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural Language Supervision},
  author={Kang, Gi-Cheon and Kim, Junghyun and Shim, Kyuhwan and Lee, Jun Ki and Zhang, Byoung-Tak},
  title = "Academic Project Page Template",
  year = "2024",
  conference = "3rd Workshop on Language and Robot Learning (LangRob) @ The Conference on Robot Learning (CoRL 2024)",
}
```

## Acknowledgements
This work was partly supported by the IITP (RS-2021-II212068-AIHub/10%, RS-2021-II211343-GSAI/20%, 2022-0-00951-LBA/20%, 2022-0-00953-PICA/20%) and NRF (RS-2024-00353991/20%, RS-2023-00274280/10%) grant funded by the Korean government.
