---
layout: ../layouts/Layout.astro
title: "CLIP-RT : Learning Language-Conditioned Robotic Policies from Natural Language Supervision"
description: Learning Language-Conditioned Robotic Policies from Natural Language Supervision
favicon: favicon.svg
thumbnail: screenshot.png
---

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import outside from "../assets/outside.mp4";
import model_overview from "../assets/model_overview.png";
import Splat from "../components/Splat.tsx"

import CodeBlock from "../components/CodeBlock.astro";
export const components = {pre: CodeBlock}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Gi-Cheon Kang",
      url: "https://gicheonkang.com",
      notes: ["*,1"],
    },
    {
      name: "Junghyun Kim",
      url:"https://jhkim-snu.github.io/",
      notes: ["*,1"],
    },
    {
      name: "Kyuhwan Shim",
      url:"https://bi.snu.ac.kr/~khshim/",
      notes: ["1"],
    },
    {
      name: "Jun Ki Lee",
      url:"https://junkilee.github.io/",
      notes: ["†,2"],
    },
    {
      name: "Byoung-Tak Zhang",
      url:"https://bi.snu.ac.kr/~btzhang/",
      notes: ["†,2"],
    },
  ]}
  institute={[
    { symbol: "¹", name: "Graduate School of AI, Seoul National University" },
    { symbol: "²", name: "AI Institute, Seoul National University (AIIS)" },
  ]}
  conference="Conference of Robot Learning Workshop@2024, LangRob"
  notes={[
    {
      symbol: "*",
      text: "Equal Contribution",
    },
    {
      symbol: "†",
      text: "Corresponding Authors",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "https://sites.google.com/view/langrob-corl24/home?authuser=0",
      icon: "fa-solid:file-pdf",
    },
    {
      name: "Code",
      url: "https://github.com/JHKim-snu/CLIP-RT",
      icon: "mdi:github",
    },
    {
      name: "arXiv",
      url: "https://arxiv.org/",
      icon: "academicons:arxiv",
    },
  ]}
/>



<Video source={outside} />

<HighlightedSection>

## Abstract

This paper explores how nonexperts can teach robots desired skills in their environments. We argue that natural language is an intuitive and accessible interface for robot learning. 
To this end, we investigate two key aspects: (1) how nonexperts collect robotic data using natural language supervision and (2) how pre-trained vision-language models learn end-to-end policies directly from this supervision. 
We propose a data collection framework that collects robot demonstrations based on natural language supervision (e.g., ``move forward'') and further augments these demonstrations. 
Next, we introduce a model that learns language-conditioned policies from natural language supervision called CLIP-RT. 
Our model employs pre-trained CLIP models and learns to predict actions represented in language via contrastive imitation learning. 
We first train CLIP-RT on large-scale robotic data and then enable it to learn desired skills using data collected from our framework.
CLIP-RT shows strong capabilities in acquiring novel manipulation skills, outperforming the state-of-the-art model, OpenVLA (7B parameters), by 17\% in average success rates, while using 7x fewer parameters (1B). 

</HighlightedSection>

## Figures

In practice, we add a simple text prompt to language instructions: What motion should the robot arm perform to complete the instruction

<Figure
    caption="Overview of CLIP-RT.">
    <Image source={model_overview} altText="Overview of CLIP-RT." />
</Figure>

## Two columns

Use the two columns component to display two columns of content. In this example, the first column contains a figure with a YouTube video and the second column contains a figure with a custom [React](https://react.dev/) component. By default, they display side by side, but if the screen is narrow enough (for example, on mobile), they're arranged vertically.

<TwoColumns>
  <Figure slot="left" caption="Take a look at this YouTube video.">
    <YouTubeVideo videoId="wjZofJX0v4M" />
  </Figure>
  <Figure slot="right" caption="Now look at this Gaussian Splat, rendered with a React component.">
    <Splat client:idle />
  </Figure>
</TwoColumns>

## Heading levels

Use headings to divide your content into sections.

### Heading 3

Go down a level to heading 3...

#### Heading 4

...and down again to heading 4.

## LaTeX

You can also add LaTeX formulas, rendered during the build process using [KaTeX](https://katex.org/) so they're quick to load for visitors of your project page. You can write them inline, like this: <LaTeX inline formula="a^2 + b^2 = c^2" />. Or, you can write them as a block:

<LaTeX formula="\int_a^b f(x) dx" />

## Tables

You can add simple tables using [GitHub Flavored Markdown syntax](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/organizing-information-with-tables):

| Model | Accuracy | F1 score | Training time (hours) |
| :--- | :---: | :---: | :---: |
| BERT-base | 0.89 | 0.87 | 4.5 |
| RoBERTa-large | 0.92 | 0.91 | 7.2 |
| DistilBERT | 0.86 | 0.84 | 2.1 |
| XLNet | 0.90 | 0.89 | 6.8 |

## BibTeX citation

```bibtex
@misc{roman2024academic,
  author = "{Roman Hauksson}",
  title = "Academic Project Page Template",
  year = "2024",
  howpublished = "\url{https://research-template.roman.technology}",
}
```